{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd793930-12a0-4af7-890b-65e667cfa11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://generativeai.pub/fine-tuning-llama-3-with-orpo-a-deep-dive-1452bc1b1838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861f2a28-a907-427f-a65f-3ae2f4398ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1220559e-7402-4464-b05f-ede1b2da3fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets peft trl -q\n",
    "%pip install ipywidgets wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e79fef-e188-441c-9eed-bef7da60e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "# from google.colab import userdata\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67485265-a186-467a-bb0d-4b8f7bbbd4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb7d97d-ccd9-42e1-af82-296f156e6e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkryptonbond\u001b[0m (\u001b[33manalyticerepo01\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# login(\"blah\")\n",
    "# wandb login\n",
    "import wandb\n",
    "# wb_token = userdata.get('wandb')\n",
    "# wb_token = 'blah' #userdata.get('wandb')\n",
    "\n",
    "wandb.login(key=wb_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d30022-ff20-447b-b9ee-005054359ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.2+cu121\n",
      "GPU detected: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(\"GPU detected:\", device_name)\n",
    "    # Enable TF32 for faster matrix multiplication on supported GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "else:\n",
    "    print(\"No GPU found. Please enable a GPU runtime for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37beece-eb18-40d5-8749-c32618d13967",
   "metadata": {},
   "source": [
    "###  Model & Dataset Preparation\n",
    "We now load the DeepSeek-R1-Distill-Qwen-1.5B model and its tokenizer from Hugging Face, and load the LIMO dataset. The dataset consists of high-quality reasoning samples with a question, a detailed solution, and the final answer.\n",
    "\n",
    "We also define a helper function format_prompt that formats the question into a prompt instructing the model to output a reasoning chain and final answer using the tags <think> and <answer>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636db18c-5626-4a59-b63f-d70a43b782c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f291a259-bedf-4341-b7fb-e6cc276fbd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091912f88a594b6dba4c852ede646222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e220ad10234ce0b8e74067eec30609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test output: What is the capital of France? I know it's Paris, but I want to\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\",\n",
    "    # Uncomment the following line if the model requires custom code\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Quick test generation\n",
    "prompt_test = \"What is the capital of France?\"\n",
    "inputs_test = tokenizer(prompt_test, return_tensors=\"pt\").to(model.device)\n",
    "outputs_test = model.generate(**inputs_test, max_new_tokens=10)\n",
    "print(\"Test output:\", tokenizer.decode(outputs_test[0], skip_special_tokens=True))\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed1f24c-7304-4c77-8a44-33fd71fe04df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 817\n",
      "Question: Find the last three digits of the product of the positive roots of $\\sqrt{1995}x^{\\log_{1995}x}=x^2.$\n",
      "Solution (excerpt): Okay, so I need to find the last three digits of the product of the positive roots of the equation âˆš...\n",
      "Answer: 25\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the LIMO dataset\n",
    "dataset = load_dataset(\"GAIR/LIMO\")\n",
    "train_data = dataset[\"train\"]\n",
    "print(\"Total training samples:\", len(train_data))\n",
    "\n",
    "# Display a sample\n",
    "sample = train_data[0]\n",
    "print(\"Question:\", sample[\"question\"])\n",
    "print(\"Solution (excerpt):\", sample[\"solution\"][:100] + \"...\")\n",
    "print(\"Answer:\", sample[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e3be73-7c2a-4413-becc-bd8e321d1249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve the following problem step by step, then give the final answer. Format your response as: [reasoning][final answer].\n",
      "Question: Find the last three digits of the product of the positive roots of $\\sqrt{1995}x^{\\log_{1995}x}=x^2.$\n",
      "Solution:\n"
     ]
    }
   ],
   "source": [
    "def format_prompt(question):\n",
    "    \"\"\"\n",
    "    Format the prompt to instruct the model to output a chain-of-thought and final answer.\n",
    "    \"\"\"\n",
    "    instruction = (\n",
    "        \"Solve the following problem step by step, then give the final answer. \"\n",
    "        \"Format your response as: [reasoning][final answer].\"\n",
    "    )\n",
    "    return f\"{instruction}\\nQuestion: {question}\\nSolution:\"\n",
    "\n",
    "# Test the formatting\n",
    "formatted_prompt = format_prompt(sample[\"question\"])\n",
    "print(formatted_prompt)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694914bb-ea29-46bb-b53f-5a4eab54eeef",
   "metadata": {},
   "source": [
    "### 3. Reinforcement Learning Fine-Tuning (GRPO)\n",
    "In this section, we implement a simplified GRPO training loop. The main steps include:\n",
    "\n",
    "Sampling: For each prompt, we generate multiple outputs (a group) from the model.\n",
    "Reward Scoring: Compute a reward for each output based on answer accuracy and proper formatting.\n",
    "Advantage Calculation: Compute the advantage by comparing each reward to the group average.\n",
    "Policy Optimization: Update the model weights using the advantage-weighted log-likelihood loss along with a KL divergence penalty to keep the model close to the reference (base) policy.\n",
    "We use a default learning rate of 1e-6, group size of 7, and a KL weight Î² = 0.04. We also set up an optimizer that supports 8-bit parameters (via bitsandbytes) for memory efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b27dd95-aac7-4ae3-829a-2cd9d43eb69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer and reward function set up.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from transformers import AdamW  # Standard AdamW\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-6\n",
    "tokens_per_generation = 4096  # Maximum tokens per generation (can be ablated)\n",
    "group_size = 7\n",
    "beta = 0.04\n",
    "\n",
    "# Initialize the 8-bit AdamW optimizer (using bitsandbytes)\n",
    "import bitsandbytes as bnb\n",
    "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Optionally, use standard 32-bit AdamW:\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Clone the initial model to serve as the reference for KL divergence\n",
    "from transformers import AutoModelForCausalLM\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(model.device)\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def reward_function(question, generated_text, true_answer):\n",
    "    \"\"\"\n",
    "    A simple rule-based reward:\n",
    "      - +0.1 bonus if output contains both  and  tags\n",
    "      - +1.0 if the extracted answer matches the true answer\n",
    "      - Small penalty if no answer is extracted\n",
    "    \"\"\"\n",
    "    answer = None\n",
    "    if \"\" in generated_text and \"\" in generated_text:\n",
    "        start = generated_text.index(\"\") + len(\"\")\n",
    "        end = generated_text.index(\"\")\n",
    "        answer = generated_text[start:end].strip()\n",
    "    else:\n",
    "        # Fallback: take the last token as the answer\n",
    "        answer = generated_text.strip().split()[-1]\n",
    "\n",
    "    reward = 0.0\n",
    "    # Bonus for proper formatting\n",
    "    if \"\" in generated_text and \"\" in generated_text and \"\" in generated_text and \"\" in generated_text:\n",
    "        reward += 0.1\n",
    "    \n",
    "    # Reward based on answer accuracy\n",
    "    if answer is not None:\n",
    "        pred_ans = answer.strip().strip('.')\n",
    "        true_ans = str(true_answer).strip().strip('.')\n",
    "        if pred_ans == true_ans:\n",
    "            reward += 1.0\n",
    "    else:\n",
    "        reward -= 0.1\n",
    "    \n",
    "    return reward\n",
    "\n",
    "print(\"Optimizer and reward function set up.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95b5c398-fb94-4c63-8186-f526698ffa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "model.train()\n",
    "max_train_steps = 2  # Demo steps; in practice, use many more steps\n",
    "grad_accum_steps = 8  # Effective batch: grad_accum_steps * group_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70bf01c1-ea5a-4398-8b37-5eb8767bcfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: policy_loss=0.0000, kl_loss=0.0000, rewards=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "Training demo completed.\n"
     ]
    }
   ],
   "source": [
    "# Shuffle training indices\n",
    "indices = list(range(len(train_data)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "step = 0\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for idx in indices[: max_train_steps * grad_accum_steps]:\n",
    "    question = train_data[idx][\"question\"]\n",
    "    true_answer = train_data[idx][\"answer\"]\n",
    "    prompt = format_prompt(question)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    # Generate a group of outputs\n",
    "    generated_texts = []\n",
    "    for _ in range(group_size):\n",
    "        output_ids = model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=200,  # For demo; in practice, use tokens_per_generation\n",
    "            do_sample=True, \n",
    "            temperature=1.0,\n",
    "            eos_token_id=tokenizer.convert_tokens_to_ids(\"\")\n",
    "        )\n",
    "        generated = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        generated_texts.append(generated)\n",
    "    \n",
    "    # Compute rewards and advantages\n",
    "    rewards = [reward_function(question, text, true_answer) for text in generated_texts]\n",
    "    baseline = sum(rewards) / len(rewards)\n",
    "    advantages = [r - baseline for r in rewards]\n",
    "    \n",
    "    # Compute policy loss\n",
    "    policy_loss = 0.0\n",
    "    for text, adv in zip(generated_texts, advantages):\n",
    "        full_text = prompt + text\n",
    "        enc = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
    "        labels = enc.input_ids.clone()\n",
    "        labels[:, :input_ids.shape[1]] = -100  # Mask prompt tokens from loss\n",
    "        out = model(**enc, labels=labels)\n",
    "        # Multiply the average loss by the number of output tokens\n",
    "        policy_loss += adv * (out.loss * labels[:, input_ids.shape[1]:].numel())\n",
    "    policy_loss = policy_loss / group_size\n",
    "    \n",
    "    # Approximate KL divergence loss\n",
    "    kl_loss = 0.0\n",
    "    for text in generated_texts:\n",
    "        full_text = prompt + text\n",
    "        enc = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
    "        labels = enc.input_ids.clone()\n",
    "        labels[:, :input_ids.shape[1]] = -100\n",
    "        with torch.no_grad():\n",
    "            curr_out = model(**enc, labels=labels)\n",
    "            ref_out = ref_model(**enc, labels=labels)\n",
    "        curr_nll = curr_out.loss * labels[:, input_ids.shape[1]:].numel()\n",
    "        ref_nll = ref_out.loss * labels[:, input_ids.shape[1]:].numel()\n",
    "        kl_loss += (curr_nll - ref_nll) / labels[:, input_ids.shape[1]:].numel()\n",
    "    kl_loss = kl_loss / group_size\n",
    "    \n",
    "    total_loss = policy_loss + beta * kl_loss\n",
    "    total_loss.backward()\n",
    "    \n",
    "    if (idx + 1) % grad_accum_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        step += 1\n",
    "        print(f\"Step {step}: policy_loss={policy_loss.item():.4f}, kl_loss={kl_loss.item():.4f}, rewards={rewards}\")\n",
    "        if step >= max_train_steps:\n",
    "            break\n",
    "\n",
    "model.eval()\n",
    "print(\"Training demo completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc27469-d805-42ca-ac97-9cf628f9de4d",
   "metadata": {},
   "source": [
    "### 4. Evaluation & Performance Metrics\n",
    "After fine-tuning, we evaluate the model on reasoning benchmarks (e.g., AIME24, GPQA, MATH-500). In this demo, we show an evaluation example for one benchmark.\n",
    "\n",
    "The process involves:\n",
    "\n",
    "Formatting the prompt as during training.\n",
    "Generating an answer using greedy decoding.\n",
    "Extracting the answer using the <answer> tags and comparing it with the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ea6d912-27e4-4df7-af12-993f7ac2eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If x + y = 10 and x - y = 2, what is the value of x?\n",
      "Predicted Answer: x\n",
      "True Answer: 6\n",
      "\n",
      "Question: Compute the area of a circle with radius 7.\n",
      "Predicted Answer: \\]\n",
      "True Answer: 153.938\n",
      "\n",
      "AIME24 Accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Example evaluation for a benchmark (e.g., AIME24)\n",
    "# For illustration, let's assume we have lists of questions and true answers\n",
    "\n",
    "aime_questions = [\n",
    "    \"If x + y = 10 and x - y = 2, what is the value of x?\",\n",
    "    \"Compute the area of a circle with radius 7.\"\n",
    "]\n",
    "aime_answers = [\n",
    "    \"6\",  # x = 6\n",
    "    \"153.938\"  # Approximate area (could be rounded)\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "for question, true_answer in zip(aime_questions, aime_answers):\n",
    "    prompt = format_prompt(question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=512)  # Greedy decoding\n",
    "    output_text = tokenizer.decode(output_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # if \"\" in output_text and \"\" in output_text:\n",
    "    #     ans = output_text.split(\"\")[1].split(\"\")[0].strip()\n",
    "    # else:\n",
    "    #     ans = output_text.strip().split()[-1]\n",
    "    # output_text = tokenizer.decode(output_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    # Removed the problematic if condition and empty split\n",
    "    ans = output_text.strip().split()[-1]\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted Answer: {ans}\")\n",
    "    print(f\"True Answer: {true_answer}\\n\")\n",
    "    \n",
    "    if str(ans).strip().strip('.') == str(true_answer).strip().strip('.'):\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(aime_questions) * 100\n",
    "print(f\"AIME24 Accuracy: {accuracy:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca399ca-4365-4c17-8834-34ee7575f401",
   "metadata": {},
   "source": [
    "### 5. Hyperparameter Ablations & Future Directions\n",
    "Hyperparameter Ablations\n",
    "Key hyperparameters that can be tuned include:\n",
    "\n",
    "Learning Rate: Our default is 1e-6, but values like 2e-6, 4e-6, or 8e-6 may be experimented with.\n",
    "Group Size: Number of outputs per prompt (default is 7). Increasing this (e.g., 14, 28, or 56) can provide a more robust reward baseline but at higher computational cost.\n",
    "KL Weight (Î²): Default is 0.04. Lower values (e.g., 0.01 or 0.001) allow the model more freedom to explore but may risk divergence.\n",
    "Future Directions\n",
    "Refining the Reward Function: Improve extraction of the final answer and consider partial rewards for nearly correct outputs.\n",
    "Adaptive KL Penalty: Use adaptive techniques to adjust Î² based on the observed KL divergence during training.\n",
    "Scaling Up: Experiment with larger models or longer generation tokens to fully exploit the reasoning capabilities.\n",
    "Distillation vs. Pretrained Models: Compare training outcomes when starting from a distilled model versus a base pretrained model.\n",
    "This concludes our step-by-step guide. Happy fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0e8ff-c4d5-4d15-97a6-b554290b7f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pyramid-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-env-pyramid-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
